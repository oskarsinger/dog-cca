\input{../../hosinger/tex/oskar_macros.tex}

\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{bbm}
\usepackage{algorithm}
\usepackage{algorithmic}

\begin{document}

\section{Intro} \label{sec:intro}
\section{Optimization Problems} \label{sec:model}

	\subsection{Objective Functions} \label{subsec:objective}
	I would like to develop rigorous probabilistic models for each of these objective functions below. The place to start is probably \cite{bach2005probabilistic}.
	
	\subsubsection{Classical Two-View CCA} \label{subsubsec:classicalcca}
	
	\begin{align*}
		\tnb{minimize}&\ \ \norm{\mbX_1 \mbPhi_1 - \mbX_2 \mbPhi_2}_F^2\\
		\tnb{subject to}&\ \  \mbPhi_1^{\top}\mbX_1^{\top}\mbX_1\mbPhi_1 = \mbI_k\\
		&\ \ \mbPhi_2^{\top}\mbX_2^{\top}\mbX_2\mbPhi_2 = \mbI_k \tn{,}
	\end{align*}
	
	\noindent where $\mbPhi_1$ and $\mbPhi_2$ are the optimization variables.
	
	\subsubsection{Complete Multiview Graph} \label{subsubsec:cgraph}
	
	\begin{align*}
		\tnb{minimize}&\ \ \sum_{i=1}^m \sum_{j=i+1}^m \norm{\mbX_i \mbPhi_i - \mbX_j \mbPhi_j}_F^2\\
		\tnb{subject to}&\ \  \mbPhi_i^{\top}\mbX_i^{\top}\mbX_i\mbPhi_i = \mbI_k\ \ i=1,\ldots,m \tn{,}
	\end{align*}
	
	\noindent where $\mbPhi_i$ are the optimization variables.
	
	\subsubsection{(Mean Field Variational?) Approximation of Complete Graph} \label{subsubsec:approxfcgraph}
	
	\begin{align*}
		\tnb{minimize}&\ \ \sum_{i=1}^m \norm{\mbX_i \mbPhi_i - \mbPsi}_F^2\\
		\tnb{subject to}&\ \  \mbPhi_i^{\top}\mbX_i^{\top}\mbX_i\mbPhi_i = \mbI_k\ \ i=1,\ldots,m \tn{,}
	\end{align*}
	
	\noindent where $\mbPhi_i$ and $\mbPsi$ are the optimization variables.

	\subsubsection{Arbitrary Dependency Graph} \label{subsubsec:arbdepgraph}
	
	\begin{align*}
		\tnb{minimize}&\ \ \sum_{(i,j) \in E}\norm{\mbX_i \mbPhi_i - \mbX_j \mbPhi_j}_F^2\\
		\tnb{subject to}&\ \  \mbPhi_i^{\top}\mbX_i^{\top}\mbX_i\mbPhi_i = \mbI_k\ \ i=1,\ldots,m \tn{,}
	\end{align*}
	
	\noindent where $E$ is the set of edges in the dependency graph between the views, and $\mbPhi_i$ are the optimization variables.
	
	\subsection{Constraints} \label{subsec:constraints}
	The classical formulation of generalized eigenvalue problems including CCA is to have quadratic equality constraints $\mbX^{\top}\mbA\mbX = \mbI$ on the parameters $\mbX$ for some quadratic form $\mbA$. The purpose of these constraints is to force the parameters to counteract/reflect the scale of the data so that the model is robust to each view having dramatically different scale.
	
	\subsubsection{Issues with Equality Constraints} \label{subsubsec:issueswithequality}
	For both the classical and projected gradient algorithms for estimating CCA parameters, to satisfy the equality constraints, we are forced to orthogonalize some basis either via Gram-Schmidt orthogonalization or matrix square-roots. For batch classical and projected gradient optimization techniques, this causes scaling issues for both computation and storage. For the stochastic projected gradient algorithm, the projections onto a non-convex feasible region (a quadratic surface) result in lack of convergence guarantees and very likely slower local convergence when this is attainable. Additionally, if we want to use the transformations as dimensionality reduction on unseen data, we might consider satisfaction of equality constraints to be overfitting.
	
	\subsubsection{Alternatives} \label{subsubsec:alternatives}
	If we want to improve convergence guarantees and reduce computation, but still account for and be robust to differences in scale between the two data views, it may be benificial to penalized the parameters for straying from the equality constraints rather than requiring exact constraint satisfaction. In fact, when using the stochastic projected gradient algorithm, violating the equality constraints by a little bit is unavoidable because we use minibatches for projecting the parameters, but the feasible region is defined in terms of the full dataset.
	
	If we define some penality function $R(\mbX) = \norm{\mbX^{\top}\mbA\mbX - \mbI}_F^2$ and use this penalty function as a proximal function or regularizer, it may result in more efficient computation and better convergence properties for parameter estimation and better generalization ability for the dimensionality reduction defined in terms of the parameters.
	
\section{Algorithms} \label{sec:alg}

	\subsection{Stochastic AppGrad} \label{subsec:appgrad}
	
	\begin{algorithm}
	\caption{Stochastic $m$-view, rank-$k$ AppGrad} \label{alg:stochappgrad}
	\begin{algorithmic}[1]
		\STATE \textbf{Input:} batch size $n$, step sizes $\eta_{1t},\ldots,\eta_{mt}$, max iter $T$, num components $k$
		\STATE \textbf{Output:} Canonical bases $\fitp{\mbPhi_1, \ldots, \mbPhi_m}$
		\STATE \textbf{Initialization:} $t=0$, $\tilde{\mbPhi}_1^{(0)} \in \mbbR^{p_1 \times k}, \ldots, \tilde{\mbPhi}_m^{(0)} \in \mbbR^{p_m \times k}$ each with entries sampled invidually from $\mcN \fitp{0,1}$, $\mbPhi_j^{(0)} = \mbf{0}$ for $j = 1,\ldots,m$
		\WHILE{$t < T$ \textbf{and} not converged}
		\STATE Receive minibatch $\mbX_1^{(t)} \in \mbbR^{n \times p_1}, \ldots, \mbX_m^{(t)} \in \mbbR^{n \times p_m}$
		\FOR{$i=1, \ldots, m$}
		\STATE $\tilde{\mbPhi_j}^{(t+1)} \leftarrow \tilde{\mbPhi_i}^{(t)} - \eta_{it}\fitp{\mbX_i^{(t)}}^{\top}\fitp{(m-1)\mbX_i^{(t)}\tilde{\mbPhi}_i^{(t)} - \sum_{j \neq i}\mbX_j^{(t)}\mbPhi_j^{(t)}}$
		\STATE SVD: $\fitp{\tilde{\mbPhi_i}^{(t+1)}}^{\top} \fitp{\frac{1}{n} {\mbX_i}^{\top}\mbX_i}\tilde{\mbPhi_i}^{(t+1)} = \mbU_i^{\top}\mbD_i\mbU_i$
		\STATE $\mbPhi_i^{(t+1)} \leftarrow \tilde{\mbPhi}_i^{(t+1)}\mbU_i^{\top}\mbD_i^{-\frac{1}{2}}\mbU_i$
		\ENDFOR
		\STATE $t \leftarrow t+1$
		\ENDWHILE
	\end{algorithmic}
	\end{algorithm}
	
	\noindent \textbf{Minibatch Management:} It should be noted that the minibatches are managed by fixed-length queues to deal with different sampling rates for different views, so the pseudocode for Algorithm~\ref{alg:stochappgrad} is not entirely faithful to my implementation. I would like to eventually give more rigorous treatment of this later, possibly using some Markov chain theory.
	
	\noindent \textbf{Parameter Updates:} It should be noted that I am not making na\"{i}ve gradient updates. This is overlooked in the pseudocode for clarity and relevance reasons. I am actually applying AdaGrad scaling with optional dual averaging and shrinkage-and-thresholding to the singular values of the parameter matrix and gradient matrix.
	
	\subsubsection{Why gradients?} \label{subsubsec:whygradients}
	Gradient-based updates allow us to easily derive stochastic and mini-batch algorithms and make incremental updates where we have complete control over the scale and influence of the incremental updates. These abilities facilitate a filter-like perspective of CCA and similar problems, and since we can control the scale/influence of each incremental update, we can control the rate of change of each parameter as a function of many quantities, including the update rates of other parameters. We can use tools from online learning and online optimization to approach these problems efficiently and with strong theoretical guarantees.
	
	\subsection{GenELinK and CCALin} \label{subsec:genelinkandccalin}
	
	\begin{algorithm}
	\caption{GenELinK} \label{alg:genelink}
	\begin{algorithmic}[1]
	\STATE \textbf{Input:} $T$, $k$, symmetric matrix $\mbA$, PSD matrix $\mbB$, subroutine $\tn{GS}_{\mbB}\fitp{\cdot}$ that performs Gram-Schmidt process, with inner product $\fitab{\cdot,\cdot}_{\mbB}$
	\STATE \textbf{Output:} top $k$ eigen-space $\mbW \in \mbbR^{d \times k}$
	\STATE $\tilde{\mbW}^{(0)} \leftarrow$ random $d \times k$ matrix with each entry i.i.d. from $\mcN \fitp{0,1}$
	\STATE $\mbW^{(0)} \leftarrow \tn{GS}_{\mbB}\fitp{\tilde{\mbW}^{(0)}}$
	\FOR{$t=0,\ldots,T-1$}
	\STATE $\mbGamma^{(t)} \leftarrow \fitp{\fitp{\mbW^{(t)}}^{\top}\mbB\mbW^{(t)}}^{-1} \fitp{\fitp{\mbW^{(t)}}^{\top}\mbA\mbW^{(t)}}$
	\STATE $\tilde{\mbW}^{(t+1)} \leftarrow \argmin{\mbW} \tn{tr}\fitp{\frac{1}{2}\mbW^{\top}\mbB\mbW - \mbW^{\top}\mbA\mbW}$
	\STATE $\fitcb{\tn{Use an optimization subroutine with initialization } \mbW^{(t)}\mbGamma^{(t)}}$
	\STATE $\mbW^{(t+1)} \leftarrow \tn{GS}_{\mbB}\fitp{\tilde{\mbW}^{(t+1)}}$
	\ENDFOR
	\RETURN $\mbW^{(T)}$
	\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
	\caption{CCALin} \label{alg:ccalin}
	\begin{algorithmic}[1]
	\STATE \textbf{Input:} $T$, $k$, data matrix $\mbX \in \mbbR^{n \times d_1}$, $\mbY \in \mbbR^{n \times d_2}$, subroutine $\tn{GS}_{\mbM}\fitp{\cdot}$ that performs Gram-Schmidt process, with inner product $\fitab{\cdot,\cdot}_{\mbM}$ where $\mbM$ is any PSD matrix
	\STATE \textbf{Output:} top $k$ canonical subspace $\mbW_x \in \mbbR^{d_1 \times k}$, $\mbW_y \in \mbbR^{d_2 \times k}$
	\STATE $\mbS_{xx} \leftarrow \mbX^{\top}\mbX/n$, $\mbS_{yy} \leftarrow \mbY^{\top}\mbY/n$, $\mbS_{xy} \leftarrow \mbX^{\top}\mbY/n$
	\STATE $\mbA \leftarrow \fitp{\begin{array}{cc}0 & \mbS_{xy} \\ \mbS_{xy}^{\top} & 0 \end{array}}$, $\mbB \leftarrow \fitp{\begin{array}{cc}\mbS_{xx} & 0 \\ 0 & \mbS_{yy} \end{array}}$
	\STATE $\fitp{\begin{array}{cc} \bar{\mbW}_x \in \mbbR^{d_1 \times k} \\\bar{\mbW}_y \in \mbbR^{d_2 \times k} \end{array}} \leftarrow \tn{GenELinK}\fitp{\mbA,\mbB, T, k}$
	\STATE $\mbU \leftarrow 2k \times k$ random Gaussian matrix
	\STATE $\tilde{\mbW}_x \leftarrow \bar{\mbW}_x \mbU$, $\tilde{\mbW}_y \leftarrow \bar{\mbW}_y \mbU$
	\STATE $\mbW_x \leftarrow \tn{GS}_{\mbS_{xx}}\fitp{\tilde{\mbW}_x}$, $\mbW_y \leftarrow \tn{GS}_{\mbS_{yy}}\fitp{\tilde{\mbW}_y}$
	\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
	\caption{GenELinK Filter} \label{alg:genelink-filter}
	\begin{algorithmic}[1]
	\STATE \textbf{Initialization:} $\tilde{\mbW}^{(0)} \leftarrow$ random $d \times k$ matrix with each entry i.i.d. from $\mcN \fitp{0,1}$, $t \leftarrow 0$
	\WHILE{True}
	\STATE \textbf{Receive} symmetric matrix $\mbA^{(t)}$, PSD matrix $\mbB^{(t)}$, subroutine $\tn{GS}_{\mbB^{(t)}}\fitp{\cdot}$ that performs Gram-Schmidt process, with inner product $\fitab{\cdot,\cdot}_{\mbB^{(t)}}$
	\STATE $\mbW^{(t)} \leftarrow \tn{GS}_{\mbB^{(t)}}\fitp{\tilde{\mbW}^{(t)}}$
	\STATE $\mbGamma^{(t)} \leftarrow \fitp{\fitp{\mbW^{(t)}}^{\top}\mbB_t\mbW^{(t)}}^{-1}\fitp{\fitp{\mbW^{(t)}}^{\top}\mbA_t\mbW^{(t)}}$
	\STATE $\tilde{\mbW}^{(t+1)} \leftarrow \argmin{\mbW} \tn{tr}\fitp{\frac{1}{2}\mbW^{\top}\mbB^{(t)}\mbW - \mbW^{\top}\mbA^{(t)}\mbW}$
	\STATE $\fitcb{\tn{Use an optimization subroutine with initialization } \mbW^{(t)}\mbGamma^{(t)}}$
	\STATE $\mbW^{(t+1)} \leftarrow \tn{GS}_{\mbB^{(t)}}\fitp{\tilde{\mbW}^{(t+1)}}$
	\STATE \textbf{yield} $\mbW^{(t+1)}$
	\STATE $t \leftarrow t + 1$
	\ENDWHILE
	\end{algorithmic}
	\end{algorithm}
	
	\begin{algorithm}
	\caption{CCALin Filter} \label{alg:ccalin-filter}
	\begin{algorithmic}[1]
	\STATE \textbf{Initialization:} $t \leftarrow 0$, $\tn{GF} \fitp{\cdot, \cdot, \cdot}$ an instance of GenELinK Filter
	\WHILE{True}
	\STATE \textbf{Receive} $\mbX^{(t)} \in \mbbR^{n \times d_1}$, $\mbY^{(t)} \in \mbbR^{n \times d_2}$
	\STATE $\mbS_{xx}^{(t)} \leftarrow \fitp{\mbX^{(t)}}^{\top}\mbX^{(t)}/n$, $\mbS_{yy}^{(t)} \leftarrow \fitp{\mbY^{(t)}}^{\top}\mbY^{(t)}/n$, $\mbS_{xy}^{(t)} \leftarrow \fitp{\mbX^{(t)}}^{\top}\mbY^{(t)}/n$
	\STATE $\mbA^{(t)} \leftarrow \fitp{\begin{array}{cc}0 & \mbS_{xy}^{(t)} \\ \fitp{\mbS_{xy}^{(t)}}^{\top} & 0 \end{array}}$, $\mbB^{(t)} \leftarrow \fitp{\begin{array}{cc}\mbS_{xx}^{(t)} & 0 \\ 0 & \mbS_{yy}^{(t)} \end{array}}$
	\STATE $\fitp{\begin{array}{cc} \bar{\mbW}_x \in \mbbR^{d_1 \times k} \\\bar{\mbW}_y \in \mbbR^{d_2 \times k} \end{array}} \leftarrow \tn{GF}\fitp{\mbA^{(t)},\mbB^{(t)}, GS_{\mbB^{(t)}}}$
	\STATE $\mbU^{(t)} \leftarrow 2k \times k$ random Gaussian matrix
	\STATE $\tilde{\mbW}_x^{(t)} \leftarrow \bar{\mbW}_x^{(t)} \mbU^{(t)}$, $\tilde{\mbW}_y^{(t)} \leftarrow \bar{\mbW}_y^{(t)} \mbU^{(t)}$
	\STATE $\mbW_x^{(t)} \leftarrow \tn{GS}_{\mbS_{xx}^{(t)}}\fitp{\tilde{\mbW}_x^{(t)}}$, $\mbW_y^{(t)} \leftarrow \tn{GS}_{\mbS_{yy}^{(t)}}\fitp{\tilde{\mbW}_y^{(t)}}$
	\STATE \textbf{yield} $\mbW_x^{(t)}$, $\mbW_y^{(t)}$
	\ENDWHILE
	\end{algorithmic}
	\end{algorithm}
	
	\subsubsection{GenELinK and CCALin Filters}
	For transforming the CCA algorithm with GenELinK subroutine into an adaptive filtering algorithm, we need to ensure that two criteria are satisfied:
	
	\begin{itemize}
		\item There is an updated set of canonical bases available for filtering unseen datapoints after each minibatch is processed.
		\item State from previous parameter updates are maintained across minibatches.
	\end{itemize}
	
	\noindent To satisfy these criteria, it will likely be necessary to make a call to the GenELinK subroutine or something like it for each minibatch. There are at least two clear options aside from what is described in Algorithm~\ref{alg:genelink-filter}.
	
	\begin{itemize}
		\item Perform the full GenELinK subroutine on each minibatch and initialize each round's GenELinK with the previous round's canonical bases.
		\item For each minibatch, perform only a single stochastic gradient step on the quadratic program inside the GenELinK subroutine before giving the output back to the CCA algorithm
	\end{itemize}
	
	\subsubsection{$m$-View CCALin}
	To derive the $m$-view variation of CCALin, it should be sufficient to receive $m$ datasets $\mbX_1,\ldots,\mbX_m$ (or minibatches for the filtering scenario) and change the generation of $\mbA$ and $\mbB$ to the following
	
	\begin{align*}
		\mbA &\leftarrow \fitp{\begin{array}{cccc}0 & \mbS_{x_1 x_2} & \cdots & \mbS_{x_1 x_m} \\ \fitp{\mbS_{x_1 x_2}}^{\top} & 0 & \ddots & \vdots \\ \vdots & \ddots & \ddots & \mbS_{x_{m-1} x_m} \\ \fitp{\mbS_{x_1 x_m}}^{\top} & \cdots & \fitp{\mbS_{x_{m-1} x_m}}^{\top} & 0 \end{array}}\\
		\mbB &\leftarrow \fitp{\begin{array}{cccc}\mbS_{x_1} & 0 & \cdots & 0 \\ 0 & \ddots & \ddots & \vdots \\ \vdots & \ddots & \ddots & 0 \\ 0 & \cdots & 0 & \mbS_{x_m} \end{array}}
	\end{align*}
	
	\noindent where $\mbS_{x_i x_j} \leftarrow \mbX_i^{\top}\mbX_j/n$ and $\mbS_{x_i} \leftarrow \mbX_i^{\top}\mbX_i/n$.
	
	\bibliographystyle{apalike}
	\bibliography{../../hosinger/tex/citations.bib}
\end{document}
